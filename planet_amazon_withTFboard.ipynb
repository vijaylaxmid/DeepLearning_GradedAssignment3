{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "planet_amazon_withTFboard.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCuZ9Re_VIUk",
        "colab_type": "text"
      },
      "source": [
        "**Importing the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwxhKcsnVI_h",
        "colab_type": "code",
        "outputId": "0d29b30c-1cb8-4769-85a5-3b0ebe394495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "import gc\n",
        "import keras as k\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard \n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ \n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import datetime"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytn90cO4Vgy2",
        "colab_type": "text"
      },
      "source": [
        "**Loading dataset**\n",
        "\n",
        "On extraction of the kaggle.json file, we upolad that file to runtime. Also, we need to go to Planet Competition(website) for the dataset and need to accept the rules for the competition. After that I have downloaded alll the dateset related files, and uploaded it to my runtime while execution. It contains the image folder as well. For testing purpose, I have taken few images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSCn45z9VsAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #! mkdir -p ~/.kaggle/\n",
        " #! mv kaggle.json ~/.kaggle/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYFdaslDVuLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path = Config.data_path()/'planet'\n",
        "# path.mkdir(parents=True, exist_ok=True)\n",
        "# path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4e6Y6gMVv89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  ! kaggle competitions download -c planet-understanding-the-amazon-from-space -f train-jpg.tar.7z -p {path}  \n",
        "#  ! kaggle competitions download -c planet-understanding-the-amazon-from-space -f train_v2.csv -p {path}  \n",
        "#  ! unzip -q -n {path}/train_v2.csv.zip -d {path}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDRCJIz-VeWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = []\n",
        "x_test = []\n",
        "y_train = []\n",
        "df_train = pd.read_csv('train_v2.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygXkb6iatWh4",
        "colab_type": "code",
        "outputId": "98504027-4fc1-43a7-9fe5-14e3eabff36d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "labels = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))\n",
        "\n",
        "label_map = {l: i for i, l in enumerate(labels)}\n",
        "inv_label_map = {i: l for l, i in label_map.items()}\n",
        "\n",
        "for f, tags in tqdm(df_train.values, miniters=1000):\n",
        "    img = cv2.imread('train_996.jpg'.format(f))\n",
        "    targets = np.zeros(17)\n",
        "    for t in tags.split(' '):\n",
        "        targets[label_map[t]] = 1 \n",
        "    x_train.append(cv2.resize(img, (32, 32)))\n",
        "    y_train.append(targets)\n",
        "    \n",
        "y_train = np.array(y_train, np.uint8)\n",
        "x_train = np.array(x_train, np.float16) / 255.\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "split = 35000\n",
        "x_train, x_valid, y_train, y_valid = x_train[:split], x_train[split:], y_train[:split], y_train[split:]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=(32, 32, 3)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(17, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', # We NEED binary here, since categorical_crossentropy l1 norms the output before calculating loss.\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=4,\n",
        "          verbose=1,\n",
        "          validation_data=(x_valid, y_valid))\n",
        "          \n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "p_valid = model.predict(x_valid, batch_size=128)\n",
        "print(y_valid)\n",
        "print(p_valid)\n",
        "print(fbeta_score(y_valid, np.array(p_valid) > 0.2, beta=2, average='samples'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/40479 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|▏         | 1000/40479 [00:01<00:57, 682.11it/s]\u001b[A\n",
            "  5%|▍         | 2000/40479 [00:02<00:56, 682.91it/s]\u001b[A\n",
            "  7%|▋         | 3000/40479 [00:04<00:54, 681.51it/s]\u001b[A\n",
            " 10%|▉         | 4000/40479 [00:05<00:53, 680.93it/s]\u001b[A\n",
            " 12%|█▏        | 5000/40479 [00:07<00:52, 680.22it/s]\u001b[A\n",
            " 15%|█▍        | 6000/40479 [00:08<00:50, 680.91it/s]\u001b[A\n",
            " 17%|█▋        | 7000/40479 [00:10<00:48, 685.29it/s]\u001b[A\n",
            " 20%|█▉        | 8000/40479 [00:11<00:47, 688.51it/s]\u001b[A\n",
            " 22%|██▏       | 9000/40479 [00:13<00:45, 686.75it/s]\u001b[A\n",
            " 25%|██▍       | 10000/40479 [00:14<00:44, 687.69it/s]\u001b[A\n",
            " 27%|██▋       | 11000/40479 [00:16<00:42, 690.36it/s]\u001b[A\n",
            " 30%|██▉       | 12000/40479 [00:17<00:41, 690.29it/s]\u001b[A\n",
            " 32%|███▏      | 13000/40479 [00:18<00:39, 691.91it/s]\u001b[A\n",
            " 35%|███▍      | 14000/40479 [00:20<00:38, 694.95it/s]\u001b[A\n",
            " 37%|███▋      | 15000/40479 [00:21<00:36, 692.47it/s]\u001b[A\n",
            " 40%|███▉      | 16000/40479 [00:23<00:35, 688.34it/s]\u001b[A\n",
            " 42%|████▏     | 17000/40479 [00:24<00:34, 682.67it/s]\u001b[A\n",
            " 44%|████▍     | 18000/40479 [00:26<00:33, 681.03it/s]\u001b[A\n",
            " 47%|████▋     | 19000/40479 [00:27<00:31, 683.02it/s]\u001b[A\n",
            " 49%|████▉     | 20000/40479 [00:29<00:29, 685.69it/s]\u001b[A\n",
            " 52%|█████▏    | 21000/40479 [00:30<00:28, 689.86it/s]\u001b[A\n",
            " 54%|█████▍    | 22000/40479 [00:32<00:26, 688.99it/s]\u001b[A\n",
            " 57%|█████▋    | 23000/40479 [00:33<00:25, 688.19it/s]\u001b[A\n",
            " 59%|█████▉    | 24000/40479 [00:34<00:24, 684.98it/s]\u001b[A\n",
            " 62%|██████▏   | 25000/40479 [00:36<00:22, 684.97it/s]\u001b[A\n",
            " 64%|██████▍   | 26000/40479 [00:37<00:21, 679.86it/s]\u001b[A\n",
            " 67%|██████▋   | 27000/40479 [00:39<00:19, 678.37it/s]\u001b[A\n",
            " 69%|██████▉   | 28000/40479 [00:40<00:18, 678.68it/s]\u001b[A\n",
            " 72%|███████▏  | 29000/40479 [00:42<00:16, 679.81it/s]\u001b[A\n",
            " 74%|███████▍  | 30000/40479 [00:43<00:15, 683.75it/s]\u001b[A\n",
            " 77%|███████▋  | 31000/40479 [00:45<00:13, 679.80it/s]\u001b[A\n",
            " 79%|███████▉  | 32000/40479 [00:46<00:12, 682.67it/s]\u001b[A\n",
            " 82%|████████▏ | 33000/40479 [00:48<00:10, 684.47it/s]\u001b[A\n",
            " 84%|████████▍ | 34000/40479 [00:49<00:09, 686.58it/s]\u001b[A\n",
            " 86%|████████▋ | 35000/40479 [00:51<00:07, 691.11it/s]\u001b[A\n",
            " 89%|████████▉ | 36000/40479 [00:52<00:06, 693.08it/s]\u001b[A\n",
            " 91%|█████████▏| 37000/40479 [00:53<00:05, 693.86it/s]\u001b[A\n",
            " 94%|█████████▍| 38000/40479 [00:55<00:03, 688.86it/s]\u001b[A\n",
            " 96%|█████████▋| 39000/40479 [00:56<00:02, 688.36it/s]\u001b[A\n",
            "100%|██████████| 40479/40479 [00:59<00:00, 685.63it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(40479, 32, 32, 3)\n",
            "(40479, 17)\n",
            "Train on 35000 samples, validate on 5479 samples\n",
            "Epoch 1/4\n",
            "35000/35000 [==============================] - 118s 3ms/step - loss: 0.2907 - accuracy: 0.8939 - val_loss: 0.2547 - val_accuracy: 0.9056\n",
            "Epoch 2/4\n",
            "35000/35000 [==============================] - 118s 3ms/step - loss: 0.2642 - accuracy: 0.9050 - val_loss: 0.2553 - val_accuracy: 0.9056\n",
            "Epoch 3/4\n",
            "35000/35000 [==============================] - 117s 3ms/step - loss: 0.2618 - accuracy: 0.9051 - val_loss: 0.2547 - val_accuracy: 0.9056\n",
            "Epoch 4/4\n",
            "35000/35000 [==============================] - 118s 3ms/step - loss: 0.2606 - accuracy: 0.9051 - val_loss: 0.2558 - val_accuracy: 0.9056\n",
            "[[1 0 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 1 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]]\n",
            "[[0.07963142 0.00379553 0.31782472 ... 0.00358364 0.06675479 0.10895309]\n",
            " [0.07963142 0.00379553 0.31782472 ... 0.00358364 0.06675479 0.10895309]\n",
            " [0.07963142 0.00379553 0.31782472 ... 0.00358364 0.06675479 0.10895309]\n",
            " ...\n",
            " [0.07963142 0.00379553 0.31782472 ... 0.00358364 0.06675479 0.10895309]\n",
            " [0.07963142 0.00379553 0.31782472 ... 0.00358364 0.06675479 0.10895309]\n",
            " [0.07963139 0.00379559 0.3178247  ... 0.00358367 0.06675477 0.10895306]]\n",
            "0.6772103880591497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwNo_Pc2scoM",
        "colab_type": "code",
        "outputId": "d27bb452-fc95-44a9-e5b4-18411fc1c327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        }
      },
      "source": [
        "!tensorboard dev upload --logdir ./logs \\"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-06 06:53:54.114786: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Data for the \"graphs\" plugin is now uploaded to TensorBoard.dev! Note that uploaded data is public. If you do not want to upload data for this plugin, use the \"--plugins\" command line argument.\n",
            "Upload started and will continue reading any new data as it's added\n",
            "to the logdir. To stop uploading, press Ctrl-C.\n",
            "View your TensorBoard live at: https://tensorboard.dev/experiment/EnreNEOOTmCEJ80VXdmFpw/\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}